# Q: G1 EDU U2 Walking - Joint Trajectory Command vs Reinforcement Learning?

## â“ á€™á€±á€¸á€á€½á€”á€ºá€¸

G1 EDU U2 á€™á€¾á€¬ walking á€œá€¯á€•á€ºá€á€¬á€€ joint trajectory command sequence á€•á€±á€¸á€á€¬á€œá€¬á€¸á‹ Reinforcement Learning á€”á€²á€· train á€‘á€¬á€¸á€á€¬á€œá€¬á€¸?

---

## âœ… á€¡á€–á€¼á€±

G1 EDU U2 á walking/locomotion á€á€Šá€º **Reinforcement Learning (RL)** á€–á€¼á€„á€·á€º train á€‘á€¬á€¸á€á€±á€¬ **neural network policy** á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€«á€á€Šá€ºá‹ Joint trajectory command sequence á€™á€»á€¬á€¸ á€á€­á€¯á€€á€ºá€›á€­á€¯á€€á€º á€•á€±á€¸á€á€¬ á€™á€Ÿá€¯á€á€ºá€•á€«á‹

---

## ğŸ§  á€¡á€œá€¯á€•á€ºá€œá€¯á€•á€ºá€•á€¯á€¶ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    High-Level Command                       â”‚
â”‚         (velocity: vx, vy, omega from SDK/API)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RL-Trained Neural Network Policy               â”‚
â”‚    (Runs on robot's onboard computer at ~500Hz)             â”‚
â”‚                                                             â”‚
â”‚   Inputs:                     Outputs:                      â”‚
â”‚   â€¢ IMU data (orientation)    â€¢ Joint position targets      â”‚
â”‚   â€¢ Joint positions           â€¢ (29 DOF commands)           â”‚
â”‚   â€¢ Joint velocities                                        â”‚
â”‚   â€¢ Velocity command                                        â”‚
â”‚   â€¢ Previous actions                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Low-Level PD Controllers                   â”‚
â”‚           (Runs on motor drivers at ~1kHz)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š RL Policy vs Trajectory Command á€”á€¾á€­á€¯á€„á€ºá€¸á€šá€¾á€¥á€ºá€á€»á€€á€º

| á€¡á€á€»á€€á€º | RL Policy (G1 á€á€¯á€¶á€¸á€á€¬) | Trajectory Command |
|-------|----------------------|-------------------|
| **Adaptability** | Real-time adapt á€œá€¯á€•á€ºá€”á€­á€¯á€„á€º | Fixed sequence |
| **Robustness** | Disturbance handle á€”á€­á€¯á€„á€º | Easily fails |
| **Terrain** | Uneven ground á€€á€­á€¯ handle á€”á€­á€¯á€„á€º | Flat ground only |
| **Training** | Simulation + Real world | Manual design |
| **Computation** | Neural network inference | Lookup/interpolation |

---

## ğŸ¯ SDK á€™á€¾ Control Flow

```cpp
// User's code (High-level command)
client.SetVelocity(vx, vy, omega);   // á€’á€® command á€•á€² á€•á€±á€¸á€›á€á€šá€º

// Robot's internal (User á€™á€™á€¼á€„á€ºá€›)
// 1. RL Policy receives: IMU, joint states, velocity cmd
// 2. RL Policy outputs: 29 joint position targets
// 3. PD controllers track those targets
// 4. Loop at ~500Hz
```

---

## ğŸ”¬ Unitree RL Training Approach

Unitree á€á€Šá€º **Sim-to-Real** approach á€€á€­á€¯ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€‘á€¬á€¸á€–á€½á€šá€ºá€›á€¾á€­á€•á€«á€á€Šá€º:

1. **Simulation Training**: Isaac Gym / MuJoCo á€á€½á€„á€º millions of steps train
2. **Domain Randomization**: Physics parameters á€™á€»á€¬á€¸ randomize
3. **Real-World Fine-tuning**: á€á€€á€šá€·á€º robot á€•á€±á€«á€ºá€á€½á€„á€º fine-tune
4. **Reward Function**: 
   - Velocity tracking reward
   - Energy efficiency
   - Stability (no falling)
   - Smooth motion

---

## ğŸ’¡ Developer á€¡á€á€½á€€á€º á€†á€­á€¯á€œá€­á€¯á€›á€„á€ºá€¸

- **High-level API** (`LocoClient`) á€€á€­á€¯ á€á€¯á€¶á€¸á€œá€»á€¾á€„á€º - velocity command á€•á€² á€•á€±á€¸á€›á€á€Šá€ºáŠ walking details á€€á€­á€¯ RL policy á€€ handle á€œá€¯á€•á€ºá€á€Šá€º
- **Low-level control** (`rt/lowcmd`) á€€á€­á€¯ á€á€¯á€¶á€¸á€œá€»á€¾á€„á€º - leg joints á€á€½á€±á€€á€­á€¯ á€€á€­á€¯á€šá€ºá€á€­á€¯á€„á€º control á€œá€¯á€•á€ºá€”á€­á€¯á€„á€ºá€á€Šá€º (RL á€€á€­á€¯ bypass á€œá€¯á€•á€º)

```cpp
// High-level: RL policy á€á€¯á€¶á€¸ (recommended for walking)
client.Move(0.5, 0.0, 0.0);  // Walk forward at 0.5 m/s

// Low-level: Direct joint control (RL bypass)
// á€€á€­á€¯á€šá€ºá€•á€­á€¯á€„á€º controller á€›á€±á€¸á€› (challenging!)
```

---

## âš ï¸ á€¡á€›á€±á€¸á€€á€¼á€®á€¸á€á€±á€¬ á€¡á€á€»á€€á€ºá€™á€»á€¬á€¸

1. **Leg control** á€¡á€á€½á€€á€º RL policy á€€á€­á€¯ á€á€¯á€¶á€¸á€á€„á€·á€ºá€á€Šá€º (á€€á€­á€¯á€šá€ºá€á€­á€¯á€„á€ºá€›á€±á€¸á€›á€”á€º á€¡á€œá€½á€”á€ºá€á€€á€ºá€á€²á€á€Šá€º)
2. **Arm control** á€¡á€á€½á€€á€º low-level/trajectory á€€á€­á€¯ á€á€¯á€¶á€¸á€”á€­á€¯á€„á€ºá€á€Šá€º (balance á€™á€œá€­á€¯)
3. **Custom walking** á€œá€­á€¯á€á€»á€„á€ºá€œá€»á€¾á€„á€º Unitree á RL framework á€€á€­á€¯ á€œá€±á€·á€œá€¬á€›á€”á€º á€œá€­á€¯á€¡á€•á€ºá€á€Šá€º

---

## ğŸ“Œ á€¡á€€á€»á€¥á€ºá€¸á€á€»á€¯á€•á€º

| Control Type | Method | Use Case |
|--------------|--------|----------|
| Walking/Running | RL Policy | Locomotion |
| Arm Movements | Trajectory/Low-level | Manipulation |
| Full Body | RL + Arm SDK | Complex tasks |

